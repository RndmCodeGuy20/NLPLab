{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['adventure',\n 'belles_lettres',\n 'editorial',\n 'fiction',\n 'government',\n 'hobbies',\n 'humor',\n 'learned',\n 'lore',\n 'mystery',\n 'news',\n 'religion',\n 'reviews',\n 'romance',\n 'science_fiction']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['austen-emma.txt',\n 'austen-persuasion.txt',\n 'austen-sense.txt',\n 'bible-kjv.txt',\n 'blake-poems.txt',\n 'bryant-stories.txt',\n 'burgess-busterbrown.txt',\n 'carroll-alice.txt',\n 'chesterton-ball.txt',\n 'chesterton-brown.txt',\n 'chesterton-thursday.txt',\n 'edgeworth-parents.txt',\n 'melville-moby_dick.txt',\n 'milton-paradise.txt',\n 'shakespeare-caesar.txt',\n 'shakespeare-hamlet.txt',\n 'shakespeare-macbeth.txt',\n 'whitman-leaves.txt']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['[', 'The', 'King', 'James', 'Bible', ']', 'The', ...]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible = gutenberg.words('bible-kjv.txt')\n",
    "\n",
    "bible"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SHANTANU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been',\n 'before',\n 'being',\n 'below',\n 'between',\n 'both',\n 'but',\n 'by',\n 'can',\n 'couldn',\n \"couldn't\",\n 'd',\n 'did',\n 'didn',\n \"didn't\",\n 'do',\n 'does',\n 'doesn',\n \"doesn't\",\n 'doing',\n 'don',\n \"don't\",\n 'down',\n 'during',\n 'each',\n 'few',\n 'for',\n 'from',\n 'further',\n 'had',\n 'hadn',\n \"hadn't\",\n 'has',\n 'hasn',\n \"hasn't\",\n 'have',\n 'haven',\n \"haven't\",\n 'having',\n 'he',\n 'her',\n 'here',\n 'hers',\n 'herself',\n 'him',\n 'himself',\n 'his',\n 'how',\n 'i',\n 'if',\n 'in',\n 'into',\n 'is',\n 'isn',\n \"isn't\",\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'just',\n 'll',\n 'm',\n 'ma',\n 'me',\n 'mightn',\n \"mightn't\",\n 'more',\n 'most',\n 'mustn',\n \"mustn't\",\n 'my',\n 'myself',\n 'needn',\n \"needn't\",\n 'no',\n 'nor',\n 'not',\n 'now',\n 'o',\n 'of',\n 'off',\n 'on',\n 'once',\n 'only',\n 'or',\n 'other',\n 'our',\n 'ours',\n 'ourselves',\n 'out',\n 'over',\n 'own',\n 're',\n 's',\n 'same',\n 'shan',\n \"shan't\",\n 'she',\n \"she's\",\n 'should',\n \"should've\",\n 'shouldn',\n \"shouldn't\",\n 'so',\n 'some',\n 'such',\n 't',\n 'than',\n 'that',\n \"that'll\",\n 'the',\n 'their',\n 'theirs',\n 'them',\n 'themselves',\n 'then',\n 'there',\n 'these',\n 'they',\n 'this',\n 'those',\n 'through',\n 'to',\n 'too',\n 'under',\n 'until',\n 'up',\n 've',\n 'very',\n 'was',\n 'wasn',\n \"wasn't\",\n 'we',\n 'were',\n 'weren',\n \"weren't\",\n 'what',\n 'when',\n 'where',\n 'which',\n 'while',\n 'who',\n 'whom',\n 'why',\n 'will',\n 'with',\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n 'y',\n 'you',\n \"you'd\",\n \"you'll\",\n \"you're\",\n \"you've\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves'}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['Shantanu', 'Mane', 'fucked', 'up', 'really', 'hard', 'yesterday', '...']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"Shantanu Mane fucked up really hard yesterday...\"\n",
    "\n",
    "tokenized_sentence = word_tokenize(test_sentence)\n",
    "\n",
    "tokenized_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "filtered_words = [w for w in tokenized_sentence if not w in stopwords]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['Shantanu', 'Mane', 'fucked', 'really', 'hard', 'yesterday', '...']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shantanu', 'Mane', 'fucked', 'up', 'really', 'hard', 'yesterday', '...'], ['Shantanu', 'Mane', 'fucked', 'really', 'hard', 'yesterday', '...']\n"
     ]
    }
   ],
   "source": [
    "print(f'{tokenized_sentence}, {filtered_words}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'Match was concluded India has won the match'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"Match was concluded!!! India has won the match.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z][^\\s]*\\b')\n",
    "\n",
    "result = tokenizer.tokenize(test_sentence)\n",
    "\n",
    "\" \".join(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "extra_stopwords = (\"I\", \"google\", \"shantanu\", \"fucked\")\n",
    "\n",
    "stopwords.update(extra_stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'I',\n 'a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been',\n 'before',\n 'being',\n 'below',\n 'between',\n 'both',\n 'but',\n 'by',\n 'can',\n 'couldn',\n \"couldn't\",\n 'd',\n 'did',\n 'didn',\n \"didn't\",\n 'do',\n 'does',\n 'doesn',\n \"doesn't\",\n 'doing',\n 'don',\n \"don't\",\n 'down',\n 'during',\n 'each',\n 'few',\n 'for',\n 'from',\n 'fucked',\n 'further',\n 'google',\n 'had',\n 'hadn',\n \"hadn't\",\n 'has',\n 'hasn',\n \"hasn't\",\n 'have',\n 'haven',\n \"haven't\",\n 'having',\n 'he',\n 'her',\n 'here',\n 'hers',\n 'herself',\n 'him',\n 'himself',\n 'his',\n 'how',\n 'i',\n 'if',\n 'in',\n 'into',\n 'is',\n 'isn',\n \"isn't\",\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'just',\n 'll',\n 'm',\n 'ma',\n 'me',\n 'mightn',\n \"mightn't\",\n 'more',\n 'most',\n 'mustn',\n \"mustn't\",\n 'my',\n 'myself',\n 'needn',\n \"needn't\",\n 'no',\n 'nor',\n 'not',\n 'now',\n 'o',\n 'of',\n 'off',\n 'on',\n 'once',\n 'only',\n 'or',\n 'other',\n 'our',\n 'ours',\n 'ourselves',\n 'out',\n 'over',\n 'own',\n 're',\n 's',\n 'same',\n 'shan',\n \"shan't\",\n 'shantanu',\n 'she',\n \"she's\",\n 'should',\n \"should've\",\n 'shouldn',\n \"shouldn't\",\n 'so',\n 'some',\n 'such',\n 't',\n 'than',\n 'that',\n \"that'll\",\n 'the',\n 'their',\n 'theirs',\n 'them',\n 'themselves',\n 'then',\n 'there',\n 'these',\n 'they',\n 'this',\n 'those',\n 'through',\n 'to',\n 'too',\n 'under',\n 'until',\n 'up',\n 've',\n 'very',\n 'was',\n 'wasn',\n \"wasn't\",\n 'we',\n 'were',\n 'weren',\n \"weren't\",\n 'what',\n 'when',\n 'where',\n 'which',\n 'while',\n 'who',\n 'whom',\n 'why',\n 'will',\n 'with',\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n 'y',\n 'you',\n \"you'd\",\n \"you'll\",\n \"you're\",\n \"you've\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves'}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "['Shantanu', 'Mane', 'really', 'hard', 'yesterday', '...']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"Shantanu Mane fucked up really hard yesterday...\"\n",
    "\n",
    "tokenized_sentence = word_tokenize(test_sentence)\n",
    "\n",
    "filtered_words = [w for w in tokenized_sentence if not w in stopwords]\n",
    "\n",
    "filtered_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\SHANTANU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[('This', 'DET'),\n ('is', 'VERB'),\n ('one', 'NUM'),\n ('simple', 'ADJ'),\n ('example', 'NOUN'),\n ('.', '.')]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"This is one simple example.\"\n",
    "tokenized_sentence = word_tokenize(test_sentence)\n",
    "\n",
    "tags = nltk.pos_tag(tokenized_sentence, tagset=\"universal\")\n",
    "\n",
    "tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
